 # Display landmark confidence scores
       # for idx, landmark in enumerate(results.pose_landmarks.landmark):
            #x, y = int(landmark.x * frame.shape[1]), int(landmark.y * frame.shape[0])
            #confidence = landmark.visibility  # Confidence score
            #cv2.putText(frame, f"{idx}: {confidence:.2f}", (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)



            #   # Display the skeletal guide with custom font and text color
  #  for i, text in enumerate(skeletal_guide):
     #   if is_horse_stance_correct(landmarks):
           # guide_text_color = (0, 255, 0)
     #   else:
            # guide_text_color = (0, 0, 255)
        #cv2.putText(frame, text, (20, 20 + 30 * i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, guide_text_color, 1)



"""


Real-time Detection Integration:
Integrate your pre-trained model with your MediaPipe real-time detection setup. This might involve converting real-time detection results (e.g., detected landmarks or features) into the format expected by your model and performing real-time inference.

Classification and Visualization:
Implement logic to classify real-time detections based on model predictions and update the user interface to provide feedback to the user. Visualization can include displaying class labels, confidence scores, or bounding boxes around detected objects.

Optimization and Performance:
Optimize the model and code for real-time performance. Techniques like batch processing, model quantization, and hardware acceleration can be applied to improve inference speed.

Error Handling and User Feedback:
Implement error handling to address potential issues, such as model loading failures or real-time detection errors. Provide user-friendly feedback when things go wrong.

Testing and Validation:
Thoroughly test your integrated system to ensure it functions correctly in a real-time environment. Validate the model's accuracy in real-world scenarios.



















import cv2
import mediapipe as mp
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import math

# Initialize MediaPipe Pose model
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)

def is_horse_stance_correct(landmarks):
    # Check if the following keypoints are detected (visible)
    keypoints_detected = (
        landmarks[11].visibility > 0.5 and  # Left hip
        landmarks[12].visibility > 0.5 and  # Right hip
        landmarks[23].visibility > 0.5 and  # Left knee
        landmarks[24].visibility > 0.5     # Right knee
    )

    if keypoints_detected:
        # Calculate angles and distances
        left_hip_y = landmarks[11].y
        left_knee_y = landmarks[23].y

        # Angle between left hip and left knee
        hip_knee_angle = abs(left_hip_y - left_knee_y)

        # Tolerance values (you may need to adjust these)
        angle_tolerance = 0.1  # Tolerance for hip-knee angle
        vertical_alignment_tolerance = 0.1  # Tolerance for vertical alignment

        # Check if the keypoints match criteria for Horse Stance
        is_correct_horse_stance = (
            hip_knee_angle < angle_tolerance and
            abs(landmarks[12].y - landmarks[24].y) < vertical_alignment_tolerance
        )
    else:
        is_correct_horse_stance = False

    return is_correct_horse_stance

# Open a webcam
cap = cv2.VideoCapture(0)

# Load the custom font
font_path = 'C:/Users/john/Downloads/fonts/Barlow/Barlow-Light.ttf'  # Path to font file
font = ImageFont.truetype(font_path, size=20)

while cap is not None and cap.isOpened():
    ret, frame = cap.read()

    if not ret:
        print("Error: Could not read a frame from the webcam.")
        break

    # Convert the frame to RGB format (MediaPipe Pose model expects RGB input)
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Process the frame to obtain pose landmarks
    results = pose.process(frame_rgb)

    # Check if any pose landmarks were detected
    if results.pose_landmarks is not None:
        landmarks = results.pose_landmarks.landmark

        # Check if the detected pose matches the Horse Stance
        is_correct_horse_stance = is_horse_stance_correct(landmarks)

        if is_correct_horse_stance:
            text_color = (0, 255, 0)  # Green text
            status_text = "Horse Stance Detected"
        else:
            text_color = (239, 93, 12)  # Orange text
            status_text = "Not in Horse Stance"

        # Draw status text using Pillow
        pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(pil_image)
        draw.text((350, 20), status_text, font=font, fill=text_color)

        # Convert the image back to OpenCV format
        frame = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)

         # Draw skeleton lines on the original frame
        mp_drawing = mp.solutions.drawing_utils
        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)

    # Display the frame
    cv2.imshow("Horse Stance Detector", frame)

    # Exit when 'q' is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the webcam and close OpenCV windows
if cap is not None:
    cap.release()
cv2.destroyAllWindows()


"""

















