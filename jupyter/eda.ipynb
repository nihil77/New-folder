{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for White Spaces\n",
    "df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the labels\n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the landmark coordinates (x and y)\n",
    "df.iloc[:, 1:] = scaler.fit_transform(df.iloc[:, 1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply data augmentation\n",
    "def augment_data(x, y):\n",
    "    # Combine x and y into landmark pairs\n",
    "    landmarks = list(zip(x, y))\n",
    "\n",
    "    # Random rotation (angle in degrees)\n",
    "    angle = np.random.uniform(-10, 10)\n",
    "    rotation_matrix = cv2.getRotationMatrix2D((0, 0), angle, 1)\n",
    "    rotated_landmarks = cv2.transform(np.array(landmarks)[None, ...], rotation_matrix)[0]\n",
    "\n",
    "    # Random translation\n",
    "    x_translation = np.random.uniform(-5, 5)\n",
    "    y_translation = np.random.uniform(-5, 5)\n",
    "    translated_landmarks = rotated_landmarks + [x_translation, y_translation]\n",
    "\n",
    "    return list(zip(*translated_landmarks))\n",
    "\n",
    "# Apply data augmentation to your dataset\n",
    "augmented_landmarks = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    x = row[1:34].tolist()  # Extract x coordinates\n",
    "    y = row[34:].tolist()    # Extract y coordinates\n",
    "    augmented_x, augmented_y = augment_data(x, y)\n",
    "    augmented_landmarks.append(augmented_x + augmented_y)\n",
    "\n",
    "# Create a new DataFrame with augmented data\n",
    "augmented_df = pd.DataFrame(augmented_landmarks, columns=df.columns[1:])\n",
    "\n",
    "# Concatenate the original and augmented DataFrames\n",
    "augmented_dataset = pd.concat([df, augmented_df])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    label        x1        y1        x2        y2        x3        y3  \\\n",
      "0       7 -0.183421  0.069315 -0.162536  0.061732 -0.155372  0.063275   \n",
      "1       0 -0.556643 -0.568918 -0.570981 -0.473468 -0.583750 -0.473002   \n",
      "2       0 -0.384387 -0.535327 -0.356010 -0.580508 -0.341002 -0.580257   \n",
      "3       0  0.484072  0.119702  0.403554  0.168772  0.372961  0.152654   \n",
      "4       0 -0.807850  1.060256 -0.778786  1.060773 -0.783660  1.064325   \n",
      "..    ...       ...       ...       ...       ...       ...       ...   \n",
      "58      4  2.199458  3.881919  2.180646  3.950854  2.179288  3.924469   \n",
      "59      4  0.132382  0.069315  0.081097 -0.080988  0.065957 -0.079732   \n",
      "60      4 -0.047051 -0.552122 -0.026388 -0.723228 -0.026859 -0.723264   \n",
      "61      5  0.749634  0.002133  0.776170 -0.080988  0.779920 -0.079732   \n",
      "62      6 -0.161889 -0.518531 -0.169702 -0.544828 -0.162512 -0.526630   \n",
      "\n",
      "          x4        y4        x5  ...       x29       y29       x30       y30  \\\n",
      "0  -0.148053  0.063435 -0.198172  ... -0.352796  0.790793  0.536681  0.694568   \n",
      "1  -0.589165 -0.474196 -0.512117  ...  0.053367 -1.275040 -0.653026 -1.272766   \n",
      "2  -0.325921 -0.599644 -0.366096  ... -0.277581  0.240684  0.583109  0.173308   \n",
      "3   0.342861  0.153040  0.480827  ... -0.375360 -0.315277 -0.008843 -0.353556   \n",
      "4  -0.781262  1.067013 -0.731149  ... -0.601007  1.036586  1.442019  1.030864   \n",
      "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
      "58  2.185567  3.898537  2.269586  ...  4.453466  1.071699  1.476840  0.515210   \n",
      "59  0.036929 -0.097855  0.101172  ...  1.399722  0.509886 -2.092282  0.268592   \n",
      "60 -0.027104 -0.743012  0.035462  ...  2.746077  0.392842 -0.838737  0.627308   \n",
      "61  0.798201 -0.062012  0.743665  ...  0.767913  1.288231  0.391595  1.159778   \n",
      "62 -0.155168 -0.527960 -0.183569  ... -0.300145  0.369433  0.414809  0.307827   \n",
      "\n",
      "         x31       y31       x32       y32       x33       y33  \n",
      "0  -0.381126  0.754676  0.542694  0.617171 -0.264514  0.719053  \n",
      "1   0.061121 -1.264021 -0.806617 -1.271993  0.064952 -1.298262  \n",
      "2  -0.213377  0.265639  0.306161  0.123653 -0.449015  0.194443  \n",
      "3  -0.548875 -0.371245  0.177143 -0.470691 -0.145906 -0.330167  \n",
      "4  -0.487875  1.135669  1.144777  1.062929 -0.785070  0.957021  \n",
      "..       ...       ...       ...       ...       ...       ...  \n",
      "58  4.453085  0.817227  1.021135  0.569411  4.315062  1.032738  \n",
      "59  1.334485  0.595455 -2.000032  0.378372  2.173534  0.491903  \n",
      "60  2.859473  0.174656 -1.430203  0.516345  2.595250  0.508128  \n",
      "61  0.770240  1.238026  0.300786  1.259274  0.776598  1.308564  \n",
      "62 -0.312501  0.345250  0.413676  0.272239 -0.290871  0.351285  \n",
      "\n",
      "[63 rows x 67 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (44, 66)\n",
      "X_val shape: (9, 66)\n",
      "X_test shape: (10, 66)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'df' is your DataFrame containing the dataset\n",
    "X = df.iloc[:, 1:]  # Extract features from all columns except the first (which is 'label')\n",
    "y = df['label']  # Extract the 'label' column as the target\n",
    "\n",
    "# Split the data into training (70%), validation (15%), and test (15%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# You can print the shapes of the resulting sets to verify the split\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 8\n"
     ]
    }
   ],
   "source": [
    "# Count the number of unique classes in the 'label' column\n",
    "num_classes = len(df['label'].unique())\n",
    "\n",
    "print(\"Number of classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "input_size = 33  # Assuming 33 landmarks as input features\n",
    "hidden_size = 32  # You can adjust this\n",
    "num_classes = 2  # Replace with the number of classes in your dataset\n",
    "\n",
    "model = NeuralNetwork(input_size, hidden_size, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define your custom dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, labels):\n",
    "        self.features = torch.tensor(df.values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'features': self.features[idx], 'label': self.labels[idx]}\n",
    "\n",
    "\n",
    "# Define your dataset and data loaders (assuming you've already prepared your dataset)\n",
    "train_dataset = MyDataset(X_train, y_train)  # Replace with your dataset\n",
    "val_dataset = MyDataset(X_val, y_val)  # Replace with your validation dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
